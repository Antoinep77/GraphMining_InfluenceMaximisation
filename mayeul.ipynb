{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.156:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1618505751278)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@6075d15c\n",
       "res0: Array[(org.apache.spark.graphx.VertexId, Boolean)] = Array((34,false), (4,false), (16,false), (22,false), (28,false), (30,false), (14,false), (32,false), (24,false), (6,false), (8,false), (12,false), (18,false), (20,false), (26,false), (10,false), (2,false), (13,false), (19,false), (15,false), (21,false), (25,false), (29,false), (11,false), (27,false), (33,false), (23,false), (1,false), (17,false), (3,false), (7,false), (9,false), (31,false), (5,false))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "independant_cascade: (graph: org.apache.spark.graphx.Graph[String,Float])org.apache.spark.graphx.Graph[String,Float]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def independant_cascade(graph:Graph[String,Float]):Graph[String,Float]={\n",
    "    def mergeMsg (m1:String, m2:String):String = {\n",
    "        if(m1==\"become_inactive\" || m2 == \"become_inactive\"){return \"become_inactive\"}\n",
    "        if (m1 == \"become_infected\" || m2 == \"become_infected\"){return \"become_infected\"}\n",
    "        return \"nothing\"\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, VD:String, A:String): String ={\n",
    "        if (VD == \"susceptible\" && A == \"become_infected\"){ return \"infected\"}\n",
    "        if(VD==\"infected\" && A == \"become_inactive\"){return \"inactive\"}\n",
    "        return VD\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[String, Float]): Iterator[(VertexId, String)]={\n",
    "        if (triplet.srcAttr == \"infected\"){\n",
    "            if (scala.util.Random.nextFloat < triplet.attr){\n",
    "                return Iterator((triplet.dstId,\"become_infected\"),(triplet.srcId,\"become_inactive\"))\n",
    "            }\n",
    "        return Iterator((triplet.srcId,\"become_inactive\"))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel(\"nothing\",Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "    .mapVertices((vid,vdata) => if(vdata == \"infected\") \"inactive\" else vdata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "37: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:37: error: type mismatch;",
      " found   : org.apache.spark.graphx.Graph[Boolean,Float]",
      " required: org.apache.spark.graphx.Graph[String,Float]",
      "       independant_cascade(graph).vertices.collect",
      "                           ^",
      ""
     ]
    }
   ],
   "source": [
    "independant_cascade(graph).vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias VD\n",
       "defined type alias ED\n",
       "defined type alias Message\n",
       "linear_threshhold: (graph: org.apache.spark.graphx.Graph[VD,ED])org.apache.spark.graphx.Graph[VD,ED]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type VD = (String,Float,Float)\n",
    "type ED = Float\n",
    "type Message = (String,Float)\n",
    "\n",
    "def linear_threshhold(graph:Graph[VD,ED]):Graph[VD,ED]={\n",
    "    def mergeMsg (m1:Message, m2:Message):Message = {\n",
    "        if(m1._1==\"become_inactive\" || m2._1 == \"become_inactive\"){return (\"become_inactive\",0f)}\n",
    "        if (m1._1 == \"become_infected\" && m2._1 == \"become_infected\"){return (\"become_infected\",m1._2+m2._2)}\n",
    "        if (m1._1 == \"become_infected\" ){return m1}\n",
    "        if (m2._1 == \"become_infected\" ){return m2}\n",
    "        return (\"nothing\",0f)\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, vdata:VD, m:Message): VD ={\n",
    "        if (m._1 == \"become_inactive\"){\n",
    "            return (\"inactive\", vdata._2, vdata._3)\n",
    "        }\n",
    "        if (m._1 == \"become_infected\"){\n",
    "            val state = vdata._1\n",
    "            val new_infection_rate = vdata._2 + m._2\n",
    "            val threshold = vdata._3\n",
    "            if (state == \"susceptible\" && new_infection_rate > threshold){\n",
    "                return (\"infected\",new_infection_rate,threshold)\n",
    "            }\n",
    "            return (state,new_infection_rate,threshold)\n",
    "        }\n",
    "        return vdata\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[VD, ED]): Iterator[(VertexId, Message)]={\n",
    "        if (triplet.srcAttr._1 == \"infected\"){\n",
    "            return Iterator((triplet.dstId,(\"become_infected\",triplet.attr)),(triplet.srcId,(\"become_inactive\",0f)))\n",
    "\n",
    "        return Iterator((triplet.srcId,(\"become_inactive\",0f)))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel((\"nothing\",0f),Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_graph_weights: [VD](graph: org.apache.spark.graphx.Graph[VD,Float])(implicit m: ClassManifest[VD])org.apache.spark.graphx.Graph[VD,Float]\n",
       "graph: org.apache.spark.graphx.Graph[VD,Float] = org.apache.spark.graphx.impl.GraphImpl@36db5af3\n",
       "res2: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(2,1,0.5812977), Edge(3,1,0.6295288), Edge(3,2,0.64329004), Edge(4,1,0.2192089), Edge(4,2,0.6613361), Edge(4,3,0.8056748), Edge(5,1,0.27176243), Edge(6,1,0.77093744), Edge(7,1,0.47805238), Edge(7,5,0.9960699), Edge(7,6,0.65685326), Edge(8,1,0.8495408), Edge(8,2,0.82857066), Edge(8,3,0.62851644), Edge(8,4,0.71172094), Edge(9,1,0.1044271), Edge(9,3,0.30305225), Edge(10,3,0.47695982), Edge(11,1,0.43643355), Edge(11,5,0.9067097), Edge(11,6,0.15488851), Edge(12,1,0.5849967), Edge(13,1,0.4426...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_graph_weights[VD](graph:Graph[VD,Float])(implicit m: ClassManifest[VD]):Graph[VD,Float] = {\n",
    "    val weight_sums_per_dst = graph.triplets.map(t => (t.dstId,t.attr))\n",
    "                            .reduceByKey( _ + _)\n",
    "    val newEdges = graph.edges.map(e => (e.dstId,e))\n",
    "         .join(weight_sums_per_dst).map{ case ((dstId,(edge,weightSum)))=>Edge(edge.srcId,edge.dstId,edge.attr/weightSum)}\n",
    "    return Graph(graph.vertices,newEdges)\n",
    "}\n",
    "\n",
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices[VD]((vid,i) =>  if(scala.util.Random.nextFloat <0.1) (\"infected\",1f, scala.util.Random.nextFloat) else (\"susceptible\",0f, scala.util.Random.nextFloat))\n",
    "graph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(3,2,0.14839081), Edge(4,2,0.15255359), Edge(7,6,0.4042399), Edge(8,2,0.19113038), Edge(8,4,0.42709345), Edge(11,6,0.09532131), Edge(13,4,0.3161904), Edge(14,2,0.2113597), Edge(14,4,0.25671613), Edge(17,6,0.5004388), Edge(18,2,0.02638042), Edge(20,2,0.0017327544), Edge(22,2,0.098358065), Edge(26,24,0.054469343), Edge(28,24,0.64549035), Edge(30,24,0.18485333), Edge(31,2,0.17009422), Edge(32,26,1.0), Edge(33,16,0.77688086), Edge(33,24,0.06842391), Edge(33,30,0.930963), Edge(33,32,0.02134499), Edge(34,10,1.0), Edge(34,14,1.0), Edge(34,16,0.22311912), Edge(34,20,1.0), Edge(34,24,0.046763178), Edge(34,28,1.0), Edge(34,30,0.069037005), Edge(34,32,0.978655), Edge(34,34,1.0), Edge(2,1,0.07486691), Edge(3,1,0.08107873), Edge(4,1,0.0282...\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_graph_weights(graph).edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(org.apache.spark.graphx.VertexId, VD)] = Array((4,(susceptible,0.5269089,0.55950093)), (16,(infected,1.0,0.916711)), (2,(inactive,0.64329004,0.48665196)), (13,(inactive,1.0,0.5158876)), (23,(infected,1.0,0.9336666)), (1,(infected,1.7578967,0.91711354)), (3,(inactive,1.0,0.10067272)), (9,(inactive,1.0,0.8100585)))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_threshhold(graph).vertices.filter(vdata => vdata._2._2 >0).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectVerticeWithMaxDegree: (graph: org.apache.spark.graphx.Graph[Boolean,Float])org.apache.spark.graphx.Graph[Boolean,Float]\n",
       "selectKVerticesWithMaxDegrees: Int => (org.apache.spark.graphx.Graph[Boolean,Float] => org.apache.spark.graphx.Graph[Boolean,Float]) = $Lambda$3299/0x00000008411b5040@67c4c95e\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectVerticeWithMaxDegree(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    val maxDegreeVertice = graph.triplets.filter(triplet => !triplet.srcAttr && !triplet.dstAttr)\n",
    "                             .map(triplet => (triplet.srcId,1))\n",
    "                             .reduceByKey(_+_)\n",
    "                             .reduce((v1,v2) => if(v1._2 >= v2._2 ) v1 else v2)._1\n",
    "    return graph.mapVertices((vid, vdata) => vid == maxDegreeVertice || vdata)\n",
    "}\n",
    "\n",
    "val selectKVerticesWithMaxDegrees = (k:Int) => Function.chain(List.fill(k)(selectVerticeWithMaxDegree _ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectKVerticeRandom: (k: Float, graph: org.apache.spark.graphx.Graph[Boolean,Float])org.apache.spark.graphx.Graph[Boolean,Float]\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectKVerticeRandom (k:Float,graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    val selected = graph.vertices.sample(false,10f/graph.vertices.count())\n",
    "    return graph.outerJoinVertices(selected) { (id, oldAttr, value) =>\n",
    "              value match {\n",
    "                case Some(_) => true\n",
    "                case None => false // None means the node was not selected\n",
    "              }\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(org.apache.spark.graphx.VertexId, (Boolean, Int))] = Array((34,(true,18)), (4,(true,3)), (22,(true,2)), (28,(true,3)), (30,(false,2)), (14,(true,4)), (32,(true,4)), (6,(false,1)), (8,(true,4)), (12,(false,1)), (18,(false,2)), (20,(false,2)), (26,(false,2)), (10,(false,1)), (2,(false,1)), (13,(false,2)), (29,(false,1)), (11,(true,3)), (33,(true,11)), (17,(false,2)), (3,(false,2)), (7,(true,3)), (9,(false,2)), (31,(false,2)), (5,(false,1)))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectKVerticesWithMaxDegrees (10) (graph).vertices.join(graph.outDegrees).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@39fa0c19\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.mapVertices((vid,selected) =>  if (selected) \"infected\" else \"susceptible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@12fb28fa\n",
       "graph_init: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@59392e06\n",
       "graph_result: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@4fcdacbf\n",
       "res9: Int = 20\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@7bfb7662\n",
       "graph_init: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@41be2133\n",
       "graph_result: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@78f8f39c\n",
       "res10: Int = 24\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Float]] = MapPartitionsRDD[9240] at filter at <console>:47\n",
       "vertices: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Boolean)] = MapPartitionsRDD[9244] at distinct at <console>:49\n",
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@1f3af1c0\n",
       "graph_init: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@3c45109d\n",
       "graph_result: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@49e9eb5c\n",
       "res58: Int = 1801\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges =  sc.textFile(\"data/soc-sign-bitcoinotc.csv\")\n",
    "                .map(f => f.split(\",\"))\n",
    "                .map(a => Edge(a(0).toLong, a(1).toLong, a(2).toLong / 10f))\n",
    "                .filter(e => (e.attr > 0))\n",
    "\n",
    "val vertices = edges.map(e => (e.srcId, false)).distinct()\n",
    "\n",
    "val graph = Graph(vertices,edges)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Float]] = MapPartitionsRDD[10744] at filter at <console>:47\n",
       "vertices: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Boolean)] = MapPartitionsRDD[10748] at distinct at <console>:49\n",
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@14d3f003\n",
       "graph_init: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@fd562bb\n",
       "graph_result: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@6678748c\n",
       "res65: Int = 1813\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges =  sc.textFile(\"data/soc-sign-bitcoinotc.csv\")\n",
    "                .map(f => f.split(\",\"))\n",
    "                .map(a => Edge(a(0).toLong, a(1).toLong, a(2).toLong / 10f))\n",
    "                .filter(e => (e.attr > 0))\n",
    "\n",
    "val vertices = edges.map(e => (e.srcId, false)).distinct()\n",
    "\n",
    "val graph = Graph(vertices,edges)\n",
    "\n",
    "val graph_init = selectKVerticeRandom (10,graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
