{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-0NFRQ5HD:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1617962733014)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\r\n",
       "import org.apache.spark.graphx._\r\n",
       "import org.apache.spark.rdd.RDD\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@5a6ef783\r\n",
       "res15: Array[(org.apache.spark.graphx.VertexId, String)] = Array((34,susceptible), (4,susceptible), (16,infected), (22,susceptible), (28,infected), (30,infected), (14,susceptible), (32,susceptible), (24,susceptible), (6,infected), (8,susceptible), (12,susceptible), (18,susceptible), (20,infected), (26,susceptible), (10,susceptible), (2,susceptible), (13,susceptible), (19,susceptible), (15,susceptible), (21,susceptible), (25,infected), (29,susceptible), (11,infected), (27,infected), (33,infected), (23,susceptible), (1,susceptible), (17,susceptible), (3,susceptible), (7,infected), (9,susceptible), (31,susceptible), (5,susceptible))\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices[String]((vid,i) =>  if(scala.util.Random.nextFloat <0.1) \"infected\" else \"susceptible\")\n",
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "independant_cascade: (graph: org.apache.spark.graphx.Graph[String,Float])org.apache.spark.graphx.Graph[String,Float]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def independant_cascade(graph:Graph[String,Float]):Graph[String,Float]={\n",
    "    def mergeMsg (m1:String, m2:String):String = {\n",
    "        if(m1==\"become_inactive\" || m2 == \"become_inactive\"){return \"become_inactive\"}\n",
    "        if (m1 == \"become_infected\" || m2 == \"become_infected\"){return \"become_infected\"}\n",
    "        return \"nothing\"\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, VD:String, A:String): String ={\n",
    "        if (VD == \"susceptible\" && A == \"become_infected\"){ return \"infected\"}\n",
    "        if(VD==\"infected\" && A == \"become_inactive\"){return \"inactive\"}\n",
    "        return VD\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[String, Float]): Iterator[(VertexId, String)]={\n",
    "        if (triplet.srcAttr == \"infected\"){\n",
    "            if (scala.util.Random.nextFloat < triplet.attr){\n",
    "                return Iterator((triplet.dstId,\"become_infected\"),(triplet.srcId,\"become_inactive\"))\n",
    "            }\n",
    "        return Iterator((triplet.srcId,\"become_inactive\"))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel(\"nothing\",Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "    .mapVertices((vid,vdata) => if(vdata == \"infected\") \"inactive\" else vdata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(org.apache.spark.graphx.VertexId, String)] = Array((34,inactive), (4,susceptible), (16,susceptible), (22,susceptible), (28,susceptible), (30,susceptible), (14,inactive), (32,inactive), (24,inactive), (6,susceptible), (8,susceptible), (12,susceptible), (18,susceptible), (20,inactive), (26,inactive), (10,inactive), (2,inactive), (13,susceptible), (19,inactive), (15,susceptible), (21,inactive), (25,inactive), (29,inactive), (11,susceptible), (27,inactive), (33,inactive), (23,susceptible), (1,inactive), (17,susceptible), (3,inactive), (7,susceptible), (9,inactive), (31,susceptible), (5,susceptible))\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independant_cascade(graph).vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias VD\r\n",
       "defined type alias ED\r\n",
       "defined type alias Message\r\n",
       "linear_threshhold: (graph: org.apache.spark.graphx.Graph[VD,ED])org.apache.spark.graphx.Graph[VD,ED]\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "type VD = (String,Float,Float)\n",
    "type ED = Float\n",
    "type Message = (String,Float)\n",
    "\n",
    "def linear_threshhold(graph:Graph[VD,ED]):Graph[VD,ED]={\n",
    "    def mergeMsg (m1:Message, m2:Message):Message = {\n",
    "        if(m1._1==\"become_inactive\" || m2._1 == \"become_inactive\"){return (\"become_inactive\",0f)}\n",
    "        if (m1._1 == \"become_infected\" && m2._1 == \"become_infected\"){return (\"become_infected\",m1._2+m2._2)}\n",
    "        if (m1._1 == \"become_infected\" ){return m1}\n",
    "        if (m2._1 == \"become_infected\" ){return m2}\n",
    "        return (\"nothing\",0f)\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, vdata:VD, m:Message): VD ={\n",
    "        if (m._1 == \"become_inactive\"){\n",
    "            return (\"inactive\", vdata._2, vdata._3)\n",
    "        }\n",
    "        if (m._1 == \"become_infected\"){\n",
    "            val state = vdata._1\n",
    "            val new_infection_rate = vdata._2 + m._2\n",
    "            val threshold = vdata._3\n",
    "            if (state == \"susceptible\" && new_infection_rate > threshold){\n",
    "                return (\"infected\",new_infection_rate,threshold)\n",
    "            }\n",
    "            return (state,new_infection_rate,threshold)\n",
    "        }\n",
    "        return vdata\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[VD, ED]): Iterator[(VertexId, Message)]={\n",
    "        if (triplet.srcAttr._1 == \"infected\"){\n",
    "            return Iterator((triplet.dstId,(\"become_infected\",triplet.attr)),(triplet.srcId,(\"become_inactive\",0f)))\n",
    "\n",
    "        return Iterator((triplet.srcId,(\"become_inactive\",0f)))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel((\"nothing\",0f),Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_graph_weights: [VD](graph: org.apache.spark.graphx.Graph[VD,Float])(implicit m: ClassManifest[VD])org.apache.spark.graphx.Graph[VD,Float]\r\n",
       "graph: org.apache.spark.graphx.Graph[VD,Float] = org.apache.spark.graphx.impl.GraphImpl@663b6e40\r\n",
       "res42: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(2,1,0.09596062), Edge(3,1,0.50865424), Edge(3,2,0.31126374), Edge(4,1,0.9875025), Edge(4,2,0.97877854), Edge(4,3,0.9131947), Edge(5,1,0.66562176), Edge(6,1,0.0850693), Edge(7,1,0.67088354), Edge(7,5,0.667345), Edge(7,6,0.7835842), Edge(8,1,0.3297881), Edge(8,2,0.8641607), Edge(8,3,0.8721171), Edge(8,4,0.42827487), Edge(9,1,0.13745981), Edge(9,3,0.8750417), Edge(10,3,0.6910226), Edge(11,1,0.19506484), Edge(11,5,0.62957555), Edge(11,6,0.3148815), Edge(12,1,0.103203), Edge(13,1,0.25128...\r\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_graph_weights[VD](graph:Graph[VD,Float])(implicit m: ClassManifest[VD]):Graph[VD,Float] = {\n",
    "    val weight_sums_per_dst = graph.triplets.map(t => (t.dstId,t.attr))\n",
    "                            .reduceByKey(( _:Float) + (_:Float))\n",
    "    val newEdges = graph.edges.map(e => (e.dstId,e))\n",
    "         .join(weight_sums_per_dst).map{ case ((dstId,(edge,weightSum)))=>Edge(edge.srcId,edge.dstId,edge.attr/weightSum)}\n",
    "    return Graph(graph.vertices,newEdges)\n",
    "}\n",
    "\n",
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices[VD]((vid,i) =>  if(scala.util.Random.nextFloat <0.1) (\"infected\",1f, scala.util.Random.nextFloat) else (\"susceptible\",0f, scala.util.Random.nextFloat))\n",
    "graph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: org.apache.spark.graphx.EdgeRDD[Float] = EdgeRDDImpl[778] at RDD at EdgeRDD.scala:41\r\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_graph_weights(graph).edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[(org.apache.spark.graphx.VertexId, VD)] = Array((20,(inactive,1.0,0.30429024)), (2,(inactive,1.0555973,0.39165944)), (1,(infected,2.5511122,0.7237318)), (9,(susceptible,0.23637855,0.23980576)), (31,(inactive,1.0,0.49187386)), (5,(inactive,1.0,0.029999077)))\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_threshhold(graph).vertices.filter(vdata => vdata._2._2 >0).collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
