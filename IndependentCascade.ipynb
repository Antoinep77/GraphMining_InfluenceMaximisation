{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@1ca08b31\n",
       "res20: Array[(org.apache.spark.graphx.VertexId, Boolean)] = Array((34,false), (4,false), (16,false), (22,false), (28,false), (30,false), (14,false), (32,false), (24,false), (6,false), (8,false), (12,false), (18,false), (20,false), (26,false), (10,false), (2,false), (13,false), (19,false), (15,false), (21,false), (25,false), (29,false), (11,false), (27,false), (33,false), (23,false), (1,false), (17,false), (3,false), (7,false), (9,false), (31,false), (5,false))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "independant_cascade: (graph: org.apache.spark.graphx.Graph[String,Float])org.apache.spark.graphx.Graph[String,Float]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def independant_cascade(graph:Graph[String,Float]):Graph[String,Float]={\n",
    "    def mergeMsg (m1:String, m2:String):String = {\n",
    "        if(m1==\"become_inactive\" || m2 == \"become_inactive\"){return \"become_inactive\"}\n",
    "        if (m1 == \"become_infected\" || m2 == \"become_infected\"){return \"become_infected\"}\n",
    "        return \"nothing\"\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, VD:String, A:String): String ={\n",
    "        if (VD == \"susceptible\" && A == \"become_infected\"){ return \"infected\"}\n",
    "        if(VD==\"infected\" && A == \"become_inactive\"){return \"inactive\"}\n",
    "        return VD\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[String, Float]): Iterator[(VertexId, String)]={\n",
    "        if (triplet.srcAttr == \"infected\"){\n",
    "            if (scala.util.Random.nextFloat < triplet.attr){\n",
    "                return Iterator((triplet.dstId,\"become_infected\"),(triplet.srcId,\"become_inactive\"))\n",
    "            }\n",
    "        return Iterator((triplet.srcId,\"become_inactive\"))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel(\"nothing\",Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "    .mapVertices((vid,vdata) => if(vdata == \"infected\") \"inactive\" else vdata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "51: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:51: error: type mismatch;",
      " found   : org.apache.spark.graphx.Graph[Boolean,Float]",
      " required: org.apache.spark.graphx.Graph[String,Float]",
      "       independant_cascade(graph).vertices.collect",
      "                           ^",
      ""
     ]
    }
   ],
   "source": [
    "independant_cascade(graph).vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type VD = (String,Float,Float)\n",
    "type ED = Float\n",
    "type Message = (String,Float)\n",
    "\n",
    "def linear_threshhold(graph:Graph[VD,ED]):Graph[VD,ED]={\n",
    "    def mergeMsg (m1:Message, m2:Message):Message = {\n",
    "        if(m1._1==\"become_inactive\" || m2._1 == \"become_inactive\"){return (\"become_inactive\",0f)}\n",
    "        if (m1._1 == \"become_infected\" && m2._1 == \"become_infected\"){return (\"become_infected\",m1._2+m2._2)}\n",
    "        if (m1._1 == \"become_infected\" ){return m1}\n",
    "        if (m2._1 == \"become_infected\" ){return m2}\n",
    "        return (\"nothing\",0f)\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, vdata:VD, m:Message): VD ={\n",
    "        if (m._1 == \"become_inactive\"){\n",
    "            return (\"inactive\", vdata._2, vdata._3)\n",
    "        }\n",
    "        if (m._1 == \"become_infected\"){\n",
    "            val state = vdata._1\n",
    "            val new_infection_rate = vdata._2 + m._2\n",
    "            val threshold = vdata._3\n",
    "            if (state == \"susceptible\" && new_infection_rate > threshold){\n",
    "                return (\"infected\",new_infection_rate,threshold)\n",
    "            }\n",
    "            return (state,new_infection_rate,threshold)\n",
    "        }\n",
    "        return vdata\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[VD, ED]): Iterator[(VertexId, Message)]={\n",
    "        if (triplet.srcAttr._1 == \"infected\"){\n",
    "            return Iterator((triplet.dstId,(\"become_infected\",triplet.attr)),(triplet.srcId,(\"become_inactive\",0f)))\n",
    "\n",
    "        return Iterator((triplet.srcId,(\"become_inactive\",0f)))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel((\"nothing\",0f),Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_graph_weights: [VD](graph: org.apache.spark.graphx.Graph[VD,Float])(implicit m: ClassManifest[VD])org.apache.spark.graphx.Graph[VD,Float]\n",
       "graph: org.apache.spark.graphx.Graph[VD,Float] = org.apache.spark.graphx.impl.GraphImpl@de479d6\n",
       "res16: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(2,1,0.7777783), Edge(3,1,0.5646414), Edge(3,2,0.0057194233), Edge(4,1,0.44419193), Edge(4,2,0.25286317), Edge(4,3,0.72412825), Edge(5,1,0.92009753), Edge(6,1,0.43213677), Edge(7,1,0.9790545), Edge(7,5,0.6421369), Edge(7,6,0.3601805), Edge(8,1,0.49752104), Edge(8,2,0.85142434), Edge(8,3,0.71274054), Edge(8,4,0.83855647), Edge(9,1,0.9577075), Edge(9,3,0.40723097), Edge(10,3,0.4917034), Edge(11,1,0.8709948), Edge(11,5,0.44130003), Edge(11,6,0.13603204), Edge(12,1,0.71998966), Edge(13,1,0....\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_graph_weights[VD](graph:Graph[VD,Float])(implicit m: ClassManifest[VD]):Graph[VD,Float] = {\n",
    "    val weight_sums_per_dst = graph.triplets.map(t => (t.dstId,t.attr))\n",
    "                            .reduceByKey( _ + _)\n",
    "    val newEdges = graph.edges.map(e => (e.dstId,e))\n",
    "         .join(weight_sums_per_dst).map{ case ((dstId,(edge,weightSum)))=>Edge(edge.srcId,edge.dstId,edge.attr/weightSum)}\n",
    "    return Graph(graph.vertices,newEdges)\n",
    "}\n",
    "\n",
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices[VD]((vid,i) =>  if(scala.util.Random.nextFloat <0.1) (\"infected\",1f, scala.util.Random.nextFloat) else (\"susceptible\",0f, scala.util.Random.nextFloat))\n",
    "graph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(3,2,0.002006809), Edge(4,2,0.088723645), Edge(7,6,0.26730868), Edge(8,2,0.29874447), Edge(8,4,0.4271791), Edge(11,6,0.100956455), Edge(13,4,0.15480156), Edge(14,2,0.15437755), Edge(14,4,0.4180193), Edge(17,6,0.63173485), Edge(18,2,0.28450096), Edge(20,2,0.013835116), Edge(22,2,0.07062683), Edge(26,24,0.24121854), Edge(28,24,0.06453526), Edge(30,24,0.03656913), Edge(31,2,0.087184705), Edge(32,26,1.0), Edge(33,16,0.5309698), Edge(33,24,0.33191332), Edge(33,30,0.508525), Edge(33,32,0.66386473), Edge(34,10,1.0), Edge(34,14,1.0), Edge(34,16,0.4690302), Edge(34,20,1.0), Edge(34,24,0.32576373), Edge(34,28,1.0), Edge(34,30,0.491475), Edge(34,32,0.33613527), Edge(34,34,1.0), Edge(2,1,0.07558371), Edge(3,1,0.054871283), Edge(4,1,0.043...\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_graph_weights(graph).edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[(org.apache.spark.graphx.VertexId, VD)] = Array((2,(susceptible,0.24847716,0.7130439)), (9,(susceptible,0.08883333,0.45760077)), (31,(inactive,1.0,0.356121)))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_threshhold(graph).vertices.filter(vdata => vdata._2._2 >0).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def selectVerticeWithMaxDegree(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    val maxDegreeVertice = graph.triplets.filter(triplet => !triplet.srcAttr && !triplet.dstAttr)\n",
    "                             .map(triplet => (triplet.srcId,1))\n",
    "                             .reduceByKey(_+_)\n",
    "                             .reduce((v1,v2) => if(v1._2 >= v2._2 ) v1 else v2)._1\n",
    "    return graph.mapVertices((vid, vdata) => vid == maxDegreeVertice || vdata)\n",
    "}\n",
    "\n",
    "val selectKVerticesWithMaxDegrees = (k:Int) => Function.chain(List.fill(k)(selectVerticeWithMaxDegree _ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectVerticeRandom(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    graph.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[(org.apache.spark.graphx.VertexId, (Boolean, Int))] = Array((34,(true,18)), (4,(true,3)), (22,(true,2)), (28,(true,3)), (30,(false,2)), (14,(true,4)), (32,(true,4)), (6,(false,1)), (8,(true,4)), (12,(false,1)), (18,(false,2)), (20,(false,2)), (26,(false,2)), (10,(false,1)), (2,(false,1)), (13,(false,2)), (29,(false,1)), (11,(true,3)), (33,(true,11)), (17,(false,2)), (3,(false,2)), (7,(true,3)), (9,(false,2)), (31,(false,2)), (5,(false,1)))\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectKVerticesWithMaxDegrees (10) (graph).vertices.join(graph.outDegrees).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@4ea135d7\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.mapVertices((vid,selected) =>  if (selected) \"infected\" else \"susceptible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@188a3dce\n",
       "graph_init: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@1d5226cf\n",
       "graph_result: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@552a8f49\n",
       "res68: Int = 22\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val edges =  sc.textFile(\"data/soc-sign-bitcoinotc.csv\")\n",
    "                .map(f => f.split(\",\"))\n",
    "                .map(a => Edge(a(0).toLong, a(1).toLong, a(2).toLong / 10f))\n",
    "                .filter(e => (e.attr > 0))\n",
    "\n",
    "val vertices = edges.map(e => (e.srcId, false)).distinct()\n",
    "\n",
    "val graph = Graph(vertices,edges)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (1000) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
