{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\r\n",
       "import org.apache.spark.graphx._\r\n",
       "import org.apache.spark.rdd.RDD\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@1ca08b31\n",
       "res20: Array[(org.apache.spark.graphx.VertexId, Boolean)] = Array((34,false), (4,false), (16,false), (22,false), (28,false), (30,false), (14,false), (32,false), (24,false), (6,false), (8,false), (12,false), (18,false), (20,false), (26,false), (10,false), (2,false), (13,false), (19,false), (15,false), (21,false), (25,false), (29,false), (11,false), (27,false), (33,false), (23,false), (1,false), (17,false), (3,false), (7,false), (9,false), (31,false), (5,false))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "independant_cascade: (graph: org.apache.spark.graphx.Graph[String,Float])org.apache.spark.graphx.Graph[String,Float]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def independant_cascade(graph:Graph[String,Float]):Graph[String,Float]={\n",
    "    def mergeMsg (m1:String, m2:String):String = {\n",
    "        if(m1==\"become_inactive\" || m2 == \"become_inactive\"){return \"become_inactive\"}\n",
    "        if (m1 == \"become_infected\" || m2 == \"become_infected\"){return \"become_infected\"}\n",
    "        return \"nothing\"\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, VD:String, A:String): String ={\n",
    "        if (VD == \"susceptible\" && A == \"become_infected\"){ return \"infected\"}\n",
    "        if(VD==\"infected\" && A == \"become_inactive\"){return \"inactive\"}\n",
    "        return VD\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[String, Float]): Iterator[(VertexId, String)]={\n",
    "        if (triplet.srcAttr == \"infected\"){\n",
    "            if (scala.util.Random.nextFloat < triplet.attr){\n",
    "                return Iterator((triplet.dstId,\"become_infected\"),(triplet.srcId,\"become_inactive\"))\n",
    "            }\n",
    "        return Iterator((triplet.srcId,\"become_inactive\"))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel(\"nothing\",Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "    .mapVertices((vid,vdata) => if(vdata == \"infected\") \"inactive\" else vdata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "51: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:51: error: type mismatch;",
      " found   : org.apache.spark.graphx.Graph[Boolean,Float]",
      " required: org.apache.spark.graphx.Graph[String,Float]",
      "       independant_cascade(graph).vertices.collect",
      "                           ^",
      ""
     ]
    }
   ],
   "source": [
    "independant_cascade(graph).vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias VD\r\n",
       "defined type alias ED\r\n",
       "defined type alias Message\r\n",
       "linear_threshhold: (graph: org.apache.spark.graphx.Graph[VD,ED])org.apache.spark.graphx.Graph[VD,ED]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type VD = (String,Float,Float)\n",
    "type ED = Float\n",
    "type Message = (String,Float)\n",
    "\n",
    "def linear_threshhold(graph:Graph[VD,ED]):Graph[VD,ED]={\n",
    "    def mergeMsg (m1:Message, m2:Message):Message = {\n",
    "        if(m1._1==\"become_inactive\" || m2._1 == \"become_inactive\"){return (\"become_inactive\",0f)}\n",
    "        if (m1._1 == \"become_infected\" && m2._1 == \"become_infected\"){return (\"become_infected\",m1._2+m2._2)}\n",
    "        if (m1._1 == \"become_infected\" ){return m1}\n",
    "        if (m2._1 == \"become_infected\" ){return m2}\n",
    "        return (\"nothing\",0f)\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, vdata:VD, m:Message): VD ={\n",
    "        if (m._1 == \"become_inactive\"){\n",
    "            return (\"inactive\", vdata._2, vdata._3)\n",
    "        }\n",
    "        if (m._1 == \"become_infected\"){\n",
    "            val state = vdata._1\n",
    "            val new_infection_rate = vdata._2 + m._2\n",
    "            val threshold = vdata._3\n",
    "            if (state == \"susceptible\" && new_infection_rate > threshold){\n",
    "                return (\"infected\",new_infection_rate,threshold)\n",
    "            }\n",
    "            return (state,new_infection_rate,threshold)\n",
    "        }\n",
    "        return vdata\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[VD, ED]): Iterator[(VertexId, Message)]={\n",
    "        if (triplet.srcAttr._1 == \"infected\"){\n",
    "            return Iterator((triplet.dstId,(\"become_infected\",triplet.attr)),(triplet.srcId,(\"become_inactive\",0f)))\n",
    "\n",
    "        return Iterator((triplet.srcId,(\"become_inactive\",0f)))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel((\"nothing\",0f),Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_graph_weights: [VD](graph: org.apache.spark.graphx.Graph[VD,Float])(implicit m: ClassManifest[VD])org.apache.spark.graphx.Graph[VD,Float]\r\n",
       "graph: org.apache.spark.graphx.Graph[VD,Float] = org.apache.spark.graphx.impl.GraphImpl@6f5cf07d\r\n",
       "res11: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(2,1,0.7104812), Edge(3,1,0.6240713), Edge(3,2,0.13820559), Edge(4,1,0.3864687), Edge(4,2,0.46350425), Edge(4,3,0.16832858), Edge(5,1,0.52650243), Edge(6,1,0.6943162), Edge(7,1,0.8280435), Edge(7,5,0.81049347), Edge(7,6,0.056300104), Edge(8,1,0.29778677), Edge(8,2,0.60662943), Edge(8,3,0.6356645), Edge(8,4,0.1484586), Edge(9,1,0.80549085), Edge(9,3,0.5572843), Edge(10,3,0.4350739), Edge(11,1,0.85606533), Edge(11,5,0.2947697), Edge(11,6,0.8379629), Edge(12,1,0.5636415), Edge(13,1,0.26...\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_graph_weights[VD](graph:Graph[VD,Float])(implicit m: ClassManifest[VD]):Graph[VD,Float] = {\n",
    "    val weight_sums_per_dst = graph.triplets.map(t => (t.dstId,t.attr))\n",
    "                            .reduceByKey( _ + _)\n",
    "    val newEdges = graph.edges.map(e => (e.dstId,e))\n",
    "         .join(weight_sums_per_dst).map{ case ((dstId,(edge,weightSum)))=>Edge(edge.srcId,edge.dstId,edge.attr/weightSum)}\n",
    "    return Graph(graph.vertices,newEdges)\n",
    "}\n",
    "\n",
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices[VD]((vid,i) =>  if(scala.util.Random.nextFloat <0.1) (\"infected\",1f, scala.util.Random.nextFloat) else (\"susceptible\",0f, scala.util.Random.nextFloat))\n",
    "graph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(3,2,0.002006809), Edge(4,2,0.088723645), Edge(7,6,0.26730868), Edge(8,2,0.29874447), Edge(8,4,0.4271791), Edge(11,6,0.100956455), Edge(13,4,0.15480156), Edge(14,2,0.15437755), Edge(14,4,0.4180193), Edge(17,6,0.63173485), Edge(18,2,0.28450096), Edge(20,2,0.013835116), Edge(22,2,0.07062683), Edge(26,24,0.24121854), Edge(28,24,0.06453526), Edge(30,24,0.03656913), Edge(31,2,0.087184705), Edge(32,26,1.0), Edge(33,16,0.5309698), Edge(33,24,0.33191332), Edge(33,30,0.508525), Edge(33,32,0.66386473), Edge(34,10,1.0), Edge(34,14,1.0), Edge(34,16,0.4690302), Edge(34,20,1.0), Edge(34,24,0.32576373), Edge(34,28,1.0), Edge(34,30,0.491475), Edge(34,32,0.33613527), Edge(34,34,1.0), Edge(2,1,0.07558371), Edge(3,1,0.054871283), Edge(4,1,0.043...\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_graph_weights(graph).edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[(org.apache.spark.graphx.VertexId, VD)] = Array((2,(susceptible,0.24847716,0.7130439)), (9,(susceptible,0.08883333,0.45760077)), (31,(inactive,1.0,0.356121)))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_threshhold(graph).vertices.filter(vdata => vdata._2._2 >0).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectVerticeWithMaxDegree: (graph: org.apache.spark.graphx.Graph[Boolean,Float])org.apache.spark.graphx.Graph[Boolean,Float]\r\n",
       "selectKVerticesWithMaxDegrees: Int => (org.apache.spark.graphx.Graph[Boolean,Float] => org.apache.spark.graphx.Graph[Boolean,Float]) = $Lambda$2987/0x0000000101000840@101724b4\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectVerticeWithMaxDegree(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    val maxDegreeVertice = graph.triplets.filter(triplet => !triplet.srcAttr && !triplet.dstAttr)\n",
    "                             .map(triplet => (triplet.srcId,1))\n",
    "                             .reduceByKey(_+_)\n",
    "                             .reduce((v1,v2) => if(v1._2 >= v2._2 ) v1 else v2)._1\n",
    "    return graph.mapVertices((vid, vdata) => vid == maxDegreeVertice || vdata)\n",
    "}\n",
    "\n",
    "val selectKVerticesWithMaxDegrees = (k:Int) => Function.chain(List.fill(k)(selectVerticeWithMaxDegree _ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectVerticeRandom(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    graph.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[(org.apache.spark.graphx.VertexId, (Boolean, Int))] = Array((34,(true,18)), (4,(true,3)), (22,(true,2)), (28,(true,3)), (30,(false,2)), (14,(true,4)), (32,(true,4)), (6,(false,1)), (8,(true,4)), (12,(false,1)), (18,(false,2)), (20,(false,2)), (26,(false,2)), (10,(false,1)), (2,(false,1)), (13,(false,2)), (29,(false,1)), (11,(true,3)), (33,(true,11)), (17,(false,2)), (3,(false,2)), (7,(true,3)), (9,(false,2)), (31,(false,2)), (5,(false,1)))\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectKVerticesWithMaxDegrees (10) (graph).vertices.join(graph.outDegrees).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@4ea135d7\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.mapVertices((vid,selected) =>  if (selected) \"infected\" else \"susceptible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@188a3dce\n",
       "graph_init: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@1d5226cf\n",
       "graph_result: org.apache.spark.graphx.Graph[String,Float] = org.apache.spark.graphx.impl.GraphImpl@552a8f49\n",
       "res68: Int = 22\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (10) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "46: error: not found: value selectKVerticesWithMaxDegrees\r",
     "output_type": "error",
     "traceback": [
      "<console>:46: error: not found: value selectKVerticesWithMaxDegrees\r",
      "       val graph_init = selectKVerticesWithMaxDegrees (1000) (graph).mapVertices(\r",
      "                        ^\r",
      "<console>:49: error: missing argument list for method normalize_graph_weights\r",
      "Unapplied methods are only converted to functions when a function type is expected.\r",
      "You can make this conversion explicit by writing `normalize_graph_weights _` or `normalize_graph_weights(_)(_)` instead of `normalize_graph_weights`.\r",
      "       val graph_result = independant_cascade(normalize_graph_weights graph_init)\r",
      "                                              ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val edges =  sc.textFile(\"data/soc-sign-bitcoinotc.csv\")\n",
    "                .map(f => f.split(\",\"))\n",
    "                .map(a => Edge(a(0).toLong, a(1).toLong, a(2).toLong / 10f))\n",
    "                .filter(e => (e.attr > 0))\n",
    "\n",
    "val vertices = edges.map(e => (e.srcId, false)).distinct()\n",
    "\n",
    "val graph = Graph(vertices,edges)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (1000) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "\n",
    "val graph_result = independant_cascade(graph_init)\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2 == \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Float]] = MapPartitionsRDD[8768] at filter at <console>:49\r\n",
       "vertices: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Boolean)] = MapPartitionsRDD[8772] at distinct at <console>:51\r\n",
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@174b25cf\r\n",
       "graph_init: org.apache.spark.graphx.Graph[(String, Float, Float),Float] = org.apache.spark.graphx.impl.GraphImpl@1b616ef8\r\n",
       "graph_result: org.apache.spark.graphx.Graph[VD,ED] = org.apache.spark.graphx.impl.GraphImpl@5370da27\r\n",
       "res46: Int = 2287\r\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges =  sc.textFile(\"data/soc-sign-bitcoinotc.csv\")\n",
    "                .map(f => f.split(\",\"))\n",
    "                .map(a => Edge(a(0).toLong, a(1).toLong, a(2).toLong / 10f))\n",
    "                .filter(e => (e.attr > 0))\n",
    "\n",
    "val vertices = edges.map(e => (e.srcId, false)).distinct()\n",
    "\n",
    "val graph = Graph(vertices,edges)\n",
    "\n",
    "val graph_init = selectKVerticesWithMaxDegrees (100) (graph).mapVertices(\n",
    "                    (vid,selected) =>  if (selected) (\"infected\",1f, randomRange(0.4f,1f)) else (\"susceptible\",0f, randomRange(0.4f,1f)))\n",
    "\n",
    "val graph_result = linear_threshhold(normalize_graph_weights(graph_init))\n",
    "\n",
    "graph_result.vertices.map(v => if (v._2._1== \"susceptible\") 0 else 1).reduce((a,b) => a+b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
