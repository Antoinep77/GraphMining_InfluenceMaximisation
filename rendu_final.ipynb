{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximisation d'influence\n",
    "\n",
    "On cherche à modéliser la propagation d'influence dans un graphe de réseau social. On compare ici deux modèles différents : l'algorithme des cascades indépendantes (independant cascade model) et le modèle des seuils linéaires (linear thresholds model)\n",
    "\n",
    "## Cascades indépendantes\n",
    "\n",
    "**Initialisation :** \n",
    "Un graphe $G = (V,E)$ et un $I \\subset V$ un sous ensemble de noeuds initialement infectés, $N = \\emptyset$ l'ensemble des noeuds inactifs.\n",
    "\n",
    "**Algorithme**\n",
    "\n",
    "Tant que $I$ n'est pas vide : \n",
    "\n",
    "&nbsp; &nbsp; Soit $u \\in I$ \n",
    "\n",
    "&nbsp; &nbsp; Pour $v \\in voisins(u)$ : \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; si $v \\notin N \\cup I $ : \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ajouter $v$ à $I$ avec une probabilité $poids(u,v)$ \n",
    "\n",
    "&nbsp; &nbsp;  $I \\gets I \\setminus \\{ u \\}$\n",
    "\n",
    "&nbsp; &nbsp;  $N \\gets N \\cup \\{ u \\}$\n",
    "\n",
    "**Sortie**\n",
    "\n",
    "$N$\n",
    "\n",
    "\n",
    "## Seuils linéaires\n",
    "\n",
    "**Initialisation :** \n",
    "Un graphe $G = (V,E)$ et un $I \\subset V$ un sous ensemble de noeuds initialement infectés\n",
    "\n",
    "**Algorithme**\n",
    "\n",
    "Tant que $I$ n'est pas stable : \n",
    "\n",
    "&nbsp; &nbsp; Soit $u \\in I$ non encore visité\n",
    "\n",
    "&nbsp; &nbsp; Pour $v \\in voisins(u)$ : \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Soit $W$ l'ensemble des voisins entrants de v infectés \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; si $\\sum_{v' \\in W} poids(v',v) > seuil(v) $ : \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $I \\gets I \\cup \\{ v \\} $\n",
    "\n",
    "&nbsp; &nbsp;  $I \\gets I \\setminus \\{ u \\}$\n",
    "\n",
    "\n",
    "**Sortie**\n",
    "\n",
    "$I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-0NFRQ5HD:4041\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1618524204852)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\r\n",
       "import org.apache.spark.graphx._\r\n",
       "import org.apache.spark.rdd.RDD\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "independant_cascade: (graph: org.apache.spark.graphx.Graph[String,Float])org.apache.spark.graphx.Graph[String,Float]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def independant_cascade(graph:Graph[String,Float]):Graph[String,Float]={\n",
    "    def mergeMsg (m1:String, m2:String):String = {\n",
    "        if(m1==\"become_inactive\" || m2 == \"become_inactive\"){return \"become_inactive\"}\n",
    "        if (m1 == \"become_infected\" || m2 == \"become_infected\"){return \"become_infected\"}\n",
    "        return \"nothing\"\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, VD:String, A:String): String ={\n",
    "        if (VD == \"susceptible\" && A == \"become_infected\"){ return \"infected\"}\n",
    "        if(VD==\"infected\" && A == \"become_inactive\"){return \"inactive\"}\n",
    "        return VD\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[String, Float]): Iterator[(VertexId, String)]={\n",
    "        if (triplet.srcAttr == \"infected\"){\n",
    "            if (scala.util.Random.nextFloat < triplet.attr){\n",
    "                return Iterator((triplet.dstId,\"become_infected\"),(triplet.srcId,\"become_inactive\"))\n",
    "            }\n",
    "        return Iterator((triplet.srcId,\"become_inactive\"))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel(\"nothing\",Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "    .mapVertices((vid,vdata) => if(vdata == \"infected\") \"inactive\" else vdata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias VD\r\n",
       "defined type alias ED\r\n",
       "defined type alias Message\r\n",
       "linear_threshhold: (graph: org.apache.spark.graphx.Graph[VD,ED])org.apache.spark.graphx.Graph[VD,ED]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type VD = (String,Float,Float)\n",
    "type ED = Float\n",
    "type Message = (String,Float)\n",
    "\n",
    "def linear_threshhold(graph:Graph[VD,ED]):Graph[VD,ED]={\n",
    "    def mergeMsg (m1:Message, m2:Message):Message = {\n",
    "        if(m1._1==\"become_inactive\" || m2._1 == \"become_inactive\"){return (\"become_inactive\",0f)}\n",
    "        if (m1._1 == \"become_infected\" && m2._1 == \"become_infected\"){return (\"become_infected\",m1._2+m2._2)}\n",
    "        if (m1._1 == \"become_infected\" ){return m1}\n",
    "        if (m2._1 == \"become_infected\" ){return m2}\n",
    "        return (\"nothing\",0f)\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, vdata:VD, m:Message): VD ={\n",
    "        if (m._1 == \"become_inactive\"){\n",
    "            return (\"inactive\", vdata._2, vdata._3)\n",
    "        }\n",
    "        if (m._1 == \"become_infected\"){\n",
    "            val state = vdata._1\n",
    "            val new_infection_rate = vdata._2 + m._2\n",
    "            val threshold = vdata._3\n",
    "            if (state == \"susceptible\" && new_infection_rate > threshold){\n",
    "                return (\"infected\",new_infection_rate,threshold)\n",
    "            }\n",
    "            return (state,new_infection_rate,threshold)\n",
    "        }\n",
    "        return vdata\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[VD, ED]): Iterator[(VertexId, Message)]={\n",
    "        if (triplet.srcAttr._1 == \"infected\"){\n",
    "            return Iterator((triplet.dstId,(\"become_infected\",triplet.attr)),(triplet.srcId,(\"become_inactive\",0f)))\n",
    "\n",
    "        return Iterator((triplet.srcId,(\"become_inactive\",0f)))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel((\"nothing\",0f),Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_graph_weights: [VD](graph: org.apache.spark.graphx.Graph[VD,Float])(implicit m: ClassManifest[VD])org.apache.spark.graphx.Graph[VD,Float]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_graph_weights[VD](graph:Graph[VD,Float])(implicit m: ClassManifest[VD]):Graph[VD,Float] = {\n",
    "    val weight_sums_per_dst = graph.triplets.map(t => (t.dstId,t.attr))\n",
    "                            .reduceByKey( _ + _)\n",
    "    val newEdges = graph.edges.map(e => (e.dstId,e))\n",
    "         .join(weight_sums_per_dst).map{ case ((dstId,(edge,weightSum)))=>Edge(edge.srcId,edge.dstId,edge.attr/weightSum)}\n",
    "    return Graph(graph.vertices,newEdges)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectVerticeWithMaxDegree: (graph: org.apache.spark.graphx.Graph[Boolean,Float])org.apache.spark.graphx.Graph[Boolean,Float]\r\n",
       "selectKVerticesWithMaxDegrees: Int => (org.apache.spark.graphx.Graph[Boolean,Float] => org.apache.spark.graphx.Graph[Boolean,Float]) = $Lambda$2169/0x0000000100d94840@67dde44f\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectVerticeWithMaxDegree(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    val maxDegreeVertice = graph.triplets.filter(triplet => !triplet.srcAttr && !triplet.dstAttr)\n",
    "                             .map(triplet => (triplet.srcId,1))\n",
    "                             .reduceByKey(_+_)\n",
    "                             .reduce((v1,v2) => if(v1._2 >= v2._2 ) v1 else v2)._1\n",
    "    return graph.mapVertices((vid, vdata) => vid == maxDegreeVertice || vdata)\n",
    "}\n",
    "\n",
    "val selectKVerticesWithMaxDegrees = (k:Int) => Function.chain(List.fill(k)(selectVerticeWithMaxDegree _ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.Ordering\r\n",
       "selectKVerticeRandom: (k: Int)(graph: org.apache.spark.graphx.Graph[Boolean,Float])org.apache.spark.graphx.Graph[Boolean,Float]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.Ordering\n",
    "def selectKVerticeRandom (k:Int)(graph:Graph[Boolean,Float]):Graph[Boolean,Float] = {\n",
    "    val graphWithRandomNodes = graph.mapVertices((vid,d) => scala.util.Random.nextFloat)\n",
    "    val threshold = graphWithRandomNodes.vertices.map(v=>v._2).takeOrdered(k)(Ordering.by[Float,Float](t=>t)).last\n",
    "    return graphWithRandomNodes.mapVertices((vid,randomVal) => randomVal <= threshold)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomRange: (a: Float, b: Float)Float\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomRange(a:Float,b:Float):Float = a + (b-a)*scala.util.Random.nextFloat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Karaté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "karate: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@4dc1b40\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val karate = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fb: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@16195722\r\n",
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Float]] = MapPartitionsRDD[31] at distinct at <console>:37\r\n",
       "facebook: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@24e88f43\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fb = GraphLoader.edgeListFile(sc,\"data/facebook_combined.txt\")\n",
    "\n",
    "val edges = fb.edges.flatMap(e => Array(Edge(e.srcId,e.dstId,1f),Edge(e.dstId,e.srcId,1f))).distinct\n",
    "val facebook = normalize_graph_weights(Graph(fb.vertices,edges)).mapVertices((vid,i)=>false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Float]] = MapPartitionsRDD[74] at filter at <console>:38\r\n",
       "vertices: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Boolean)] = MapPartitionsRDD[78] at distinct at <console>:40\r\n",
       "bitcoin: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@4fb045e8\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges =  sc.textFile(\"data/soc-sign-bitcoinotc.csv\")\n",
    "                .map(f => f.split(\",\"))\n",
    "                .map(a => Edge(a(0).toLong, a(1).toLong, a(2).toLong / 10f))\n",
    "                .filter(e => (e.attr > 0))\n",
    "\n",
    "val vertices = edges.flatMap(e => Array((e.srcId, false),(e.dstId,false))).distinct()\n",
    "\n",
    "val bitcoin = Graph(vertices,edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_IC: (graph: org.apache.spark.graphx.Graph[Boolean,Float], selectFunc: Function[org.apache.spark.graphx.Graph[Boolean,Float],org.apache.spark.graphx.Graph[Boolean,Float]])Long\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_IC(graph:Graph[Boolean,Float],selectFunc:Function[Graph[Boolean,Float],Graph[Boolean,Float]]):Long = {\n",
    "    val graph_init = selectFunc(graph).mapVertices((vid,selected) =>  if (selected) \"infected\" else \"susceptible\")\n",
    "    independant_cascade(graph_init).vertices.filter(v => v._2 != \"susceptible\").count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_LT: (graph: org.apache.spark.graphx.Graph[Boolean,Float], selectFunc: Function[org.apache.spark.graphx.Graph[Boolean,Float],org.apache.spark.graphx.Graph[Boolean,Float]])Long\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_LT(graph:Graph[Boolean,Float],selectFunc:Function[Graph[Boolean,Float],Graph[Boolean,Float]]):Long = {\n",
    "    val graph_init = selectFunc(graph)\n",
    "                        .mapVertices((vid,selected) => if (selected) \n",
    "                                                             (\"infected\",1f, randomRange(0.3f,0.9f)) \n",
    "                                                       else (\"susceptible\",0f, randomRange(0.3f,0.9f)))\n",
    "    val normalized_graph_init = normalize_graph_weights(graph_init)\n",
    "    linear_threshhold(normalized_graph_init).vertices.filter(v => v._2._1 != \"susceptible\").count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facebook_IC_degrees: Array[Long] = Array(156, 287, 710, 793, 861, 827, 868, 1224)\r\n",
       "facebook_IC_random: Array[Long] = Array(1, 40, 37, 30, 86, 186, 366, 709)\r\n",
       "facebook_LT_degrees: Array[Long] = Array(19, 32, 87, 101, 122, 119, 151, 225)\r\n",
       "facebook_LT_random: Array[Long] = Array(1, 2, 5, 10, 20, 30, 50, 101)\r\n",
       "bitcoin_IC_degrees: Array[Long] = Array(1769, 1710, 1763, 1781, 1815, 1789, 1758, 1760)\r\n",
       "bitcoin_IC_random: Array[Long] = Array(1807, 1707, 1748, 1706, 1798, 1771, 1789, 1858)\r\n",
       "bitcoin_LT_degrees: Array[Long] = Array(360, 455, 627, 830, 1165, 1390, 2169, 3253)\r\n",
       "bitcoin_LT_random: Array[Long] = Array(1, 2, 6, 21, 26, 44, 72, 169)\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val facebook_IC_degrees = Array(1,2,5,10,20,30,50,100).map(k=>test_IC(facebook,selectKVerticesWithMaxDegrees(k)))\n",
    "val facebook_IC_random = Array(1,2,5,10,20,30,50,100).map(k=>test_IC(facebook,selectKVerticeRandom(k)))\n",
    "val facebook_LT_degrees = Array(1,2,5,10,20,30,50,100).map(k=>test_LT(facebook,selectKVerticesWithMaxDegrees(k)))\n",
    "val facebook_LT_random = Array(1,2,5,10,20,30,50,100).map(k=>test_LT(facebook,selectKVerticeRandom(k)))\n",
    "val bitcoin_IC_degrees = Array(1,2,5,10,20,30,50,100).map(k=>test_IC(bitcoin,selectKVerticesWithMaxDegrees(k)))\n",
    "val bitcoin_IC_random = Array(1,2,5,10,20,30,50,100).map(k=>test_IC(bitcoin,selectKVerticeRandom(k)))\n",
    "val bitcoin_LT_degrees = Array(1,2,5,10,20,30,50,100).map(k=>test_LT(bitcoin,selectKVerticesWithMaxDegrees(k)))\n",
    "val bitcoin_LT_random = Array(1,2,5,10,20,30,50,100).map(k=>test_LT(bitcoin,selectKVerticeRandom(k)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
