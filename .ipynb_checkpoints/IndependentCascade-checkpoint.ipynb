{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-0NFRQ5HD:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1617962733014)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\r\n",
       "import org.apache.spark.graphx._\r\n",
       "import org.apache.spark.rdd.RDD\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph: org.apache.spark.graphx.Graph[Boolean,Float] = org.apache.spark.graphx.impl.GraphImpl@490eb9fe\r\n",
       "res82: Array[(org.apache.spark.graphx.VertexId, Boolean)] = Array((34,false), (4,false), (16,false), (22,false), (28,false), (30,false), (14,false), (32,false), (24,false), (6,false), (8,false), (12,false), (18,false), (20,false), (26,false), (10,false), (2,false), (13,false), (19,false), (15,false), (21,false), (25,false), (29,false), (11,false), (27,false), (33,false), (23,false), (1,false), (17,false), (3,false), (7,false), (9,false), (31,false), (5,false))\r\n"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices((vid,data) =>  false)\n",
    "graph.vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "independant_cascade: (graph: org.apache.spark.graphx.Graph[String,Float])org.apache.spark.graphx.Graph[String,Float]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def independant_cascade(graph:Graph[String,Float]):Graph[String,Float]={\n",
    "    def mergeMsg (m1:String, m2:String):String = {\n",
    "        if(m1==\"become_inactive\" || m2 == \"become_inactive\"){return \"become_inactive\"}\n",
    "        if (m1 == \"become_infected\" || m2 == \"become_infected\"){return \"become_infected\"}\n",
    "        return \"nothing\"\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, VD:String, A:String): String ={\n",
    "        if (VD == \"susceptible\" && A == \"become_infected\"){ return \"infected\"}\n",
    "        if(VD==\"infected\" && A == \"become_inactive\"){return \"inactive\"}\n",
    "        return VD\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[String, Float]): Iterator[(VertexId, String)]={\n",
    "        if (triplet.srcAttr == \"infected\"){\n",
    "            if (scala.util.Random.nextFloat < triplet.attr){\n",
    "                return Iterator((triplet.dstId,\"become_infected\"),(triplet.srcId,\"become_inactive\"))\n",
    "            }\n",
    "        return Iterator((triplet.srcId,\"become_inactive\"))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel(\"nothing\",Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "    .mapVertices((vid,vdata) => if(vdata == \"infected\") \"inactive\" else vdata)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(org.apache.spark.graphx.VertexId, String)] = Array((34,inactive), (4,susceptible), (16,susceptible), (22,susceptible), (28,susceptible), (30,susceptible), (14,inactive), (32,inactive), (24,inactive), (6,susceptible), (8,susceptible), (12,susceptible), (18,susceptible), (20,inactive), (26,inactive), (10,inactive), (2,inactive), (13,susceptible), (19,inactive), (15,susceptible), (21,inactive), (25,inactive), (29,inactive), (11,susceptible), (27,inactive), (33,inactive), (23,susceptible), (1,inactive), (17,susceptible), (3,inactive), (7,susceptible), (9,inactive), (31,susceptible), (5,susceptible))\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independant_cascade(graph).vertices.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias VD\r\n",
       "defined type alias ED\r\n",
       "defined type alias Message\r\n",
       "linear_threshhold: (graph: org.apache.spark.graphx.Graph[VD,ED])org.apache.spark.graphx.Graph[VD,ED]\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "type VD = (String,Float,Float)\n",
    "type ED = Float\n",
    "type Message = (String,Float)\n",
    "\n",
    "def linear_threshhold(graph:Graph[VD,ED]):Graph[VD,ED]={\n",
    "    def mergeMsg (m1:Message, m2:Message):Message = {\n",
    "        if(m1._1==\"become_inactive\" || m2._1 == \"become_inactive\"){return (\"become_inactive\",0f)}\n",
    "        if (m1._1 == \"become_infected\" && m2._1 == \"become_infected\"){return (\"become_infected\",m1._2+m2._2)}\n",
    "        if (m1._1 == \"become_infected\" ){return m1}\n",
    "        if (m2._1 == \"become_infected\" ){return m2}\n",
    "        return (\"nothing\",0f)\n",
    "    }\n",
    "\n",
    "    def vprog(VertexId:VertexId, vdata:VD, m:Message): VD ={\n",
    "        if (m._1 == \"become_inactive\"){\n",
    "            return (\"inactive\", vdata._2, vdata._3)\n",
    "        }\n",
    "        if (m._1 == \"become_infected\"){\n",
    "            val state = vdata._1\n",
    "            val new_infection_rate = vdata._2 + m._2\n",
    "            val threshold = vdata._3\n",
    "            if (state == \"susceptible\" && new_infection_rate > threshold){\n",
    "                return (\"infected\",new_infection_rate,threshold)\n",
    "            }\n",
    "            return (state,new_infection_rate,threshold)\n",
    "        }\n",
    "        return vdata\n",
    "    }\n",
    "\n",
    "    def sendMsg(triplet:EdgeTriplet[VD, ED]): Iterator[(VertexId, Message)]={\n",
    "        if (triplet.srcAttr._1 == \"infected\"){\n",
    "            return Iterator((triplet.dstId,(\"become_infected\",triplet.attr)),(triplet.srcId,(\"become_inactive\",0f)))\n",
    "\n",
    "        return Iterator((triplet.srcId,(\"become_inactive\",0f)))\n",
    "        }\n",
    "        return Iterator()\n",
    "    }\n",
    "    \n",
    "    return graph.pregel((\"nothing\",0f),Int.MaxValue,EdgeDirection.Out)(vprog,sendMsg,mergeMsg)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_graph_weights: [VD](graph: org.apache.spark.graphx.Graph[VD,Float])(implicit m: ClassManifest[VD])org.apache.spark.graphx.Graph[VD,Float]\r\n",
       "graph: org.apache.spark.graphx.Graph[VD,Float] = org.apache.spark.graphx.impl.GraphImpl@42ba2aa7\r\n",
       "res60: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(2,1,0.070907235), Edge(3,1,0.10024041), Edge(3,2,0.051920712), Edge(4,1,0.28428066), Edge(4,2,0.60360086), Edge(4,3,0.9406253), Edge(5,1,0.122498274), Edge(6,1,0.5875056), Edge(7,1,0.2996779), Edge(7,5,0.73628646), Edge(7,6,0.549187), Edge(8,1,0.7103159), Edge(8,2,0.10904366), Edge(8,3,0.020172894), Edge(8,4,0.32169902), Edge(9,1,0.1611849), Edge(9,3,0.6805909), Edge(10,3,0.2593537), Edge(11,1,0.6347268), Edge(11,5,0.33655518), Edge(11,6,0.15796006), Edge(12,1,0.6241273), Edge(13,1,...\r\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_graph_weights[VD](graph:Graph[VD,Float])(implicit m: ClassManifest[VD]):Graph[VD,Float] = {\n",
    "    val weight_sums_per_dst = graph.triplets.map(t => (t.dstId,t.attr))\n",
    "                            .reduceByKey( _ + _)\n",
    "    val newEdges = graph.edges.map(e => (e.dstId,e))\n",
    "         .join(weight_sums_per_dst).map{ case ((dstId,(edge,weightSum)))=>Edge(edge.srcId,edge.dstId,edge.attr/weightSum)}\n",
    "    return Graph(graph.vertices,newEdges)\n",
    "}\n",
    "\n",
    "val graph = GraphLoader.edgeListFile(sc,\"data/soc-karate.mtx\")\n",
    "                .mapEdges(e => scala.util.Random.nextFloat)\n",
    "                .mapVertices[VD]((vid,i) =>  if(scala.util.Random.nextFloat <0.1) (\"infected\",1f, scala.util.Random.nextFloat) else (\"susceptible\",0f, scala.util.Random.nextFloat))\n",
    "graph.edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: Array[org.apache.spark.graphx.Edge[Float]] = Array(Edge(3,2,0.02067454), Edge(4,2,0.24035051), Edge(7,6,0.39217466), Edge(8,2,0.04342058), Edge(8,4,0.24146138), Edge(11,6,0.11279934), Edge(13,4,0.22865711), Edge(14,2,0.03924461), Edge(14,4,0.52988154), Edge(17,6,0.495026), Edge(18,2,0.23011978), Edge(20,2,0.1823213), Edge(22,2,0.15113403), Edge(26,24,0.29384622), Edge(28,24,0.19369376), Edge(30,24,0.3089109), Edge(31,2,0.09273456), Edge(32,26,1.0), Edge(33,16,0.44138572), Edge(33,24,0.16643305), Edge(33,30,0.5094223), Edge(33,32,0.5581677), Edge(34,10,1.0), Edge(34,14,1.0), Edge(34,16,0.55861425), Edge(34,20,1.0), Edge(34,24,0.0371161), Edge(34,28,1.0), Edge(34,30,0.49057773), Edge(34,32,0.4418323), Edge(34,34,1.0), Edge(2,1,0.012187461), Edge(3,1,0.017229216), Edge(4,1,0.0488618...\r\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_graph_weights(graph).edges.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[(org.apache.spark.graphx.VertexId, VD)] = Array((20,(inactive,1.0,0.30429024)), (2,(inactive,1.0555973,0.39165944)), (1,(infected,2.5511122,0.7237318)), (9,(susceptible,0.23637855,0.23980576)), (31,(inactive,1.0,0.49187386)), (5,(inactive,1.0,0.029999077)))\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_threshhold(graph).vertices.filter(vdata => vdata._2._2 >0).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectVerticeWithMaxDegree: (graph: org.apache.spark.graphx.Graph[Boolean, _])org.apache.spark.graphx.Graph[Boolean, _]\r\n",
       "selectKVerticesWithMaxDegrees: Int => (org.apache.spark.graphx.Graph[Boolean, _] => org.apache.spark.graphx.Graph[Boolean, _]) = $Lambda$3934/0x000000010137d040@6e12ec5f\r\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectVerticeWithMaxDegree(graph:Graph[Boolean,_]):Graph[Boolean,_] = {\n",
    "    val maxDegreeVertice = graph.triplets.filter(triplet => !triplet.srcAttr && !triplet.dstAttr)\n",
    "                             .map(triplet => (triplet.srcId,1))\n",
    "                             .reduceByKey(_+_)\n",
    "                             .reduce((v1,v2) => if(v1._2 >= v2._2 ) v1 else v2)._1\n",
    "    return graph.mapVertices((vid, vdata) => vid == maxDegreeVertice || vdata)\n",
    "}\n",
    "\n",
    "val selectKVerticesWithMaxDegrees = (k:Int) => Function.chain(List.fill(k)(selectVerticeWithMaxDegree _ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res88: Array[(org.apache.spark.graphx.VertexId, (Boolean, Int))] = Array((34,(true,18)), (4,(true,3)), (22,(true,2)), (28,(true,3)), (30,(false,2)), (14,(true,4)), (32,(true,4)), (6,(false,1)), (8,(true,4)), (12,(false,1)), (18,(false,2)), (20,(false,2)), (26,(false,2)), (10,(false,1)), (2,(false,1)), (13,(false,2)), (29,(false,1)), (11,(true,3)), (33,(true,11)), (17,(false,2)), (3,(false,2)), (7,(true,3)), (9,(false,2)), (31,(false,2)), (5,(false,1)))\r\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectKVerticesWithMaxDegrees (10) (graph).vertices.join(graph.outDegrees).collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
